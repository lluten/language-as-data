{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ad57ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers import models, trainers, normalizers, pre_tokenizers, processors, decoders\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62136d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/slk_newscrawl_2016_1M/slk_newscrawl_2016_1M-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    slk_df = pd.DataFrame(\n",
    "            [line.strip().split(\"\\t\")[1] for line in f.readlines()], columns=[\"sentence\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0256192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function applied to dataset cleans sentences from digits and special characters\n",
    "def produce_clean_sentence_list(df):\n",
    "    cleaned_sentences = []\n",
    "    for sentence in df[\"sentence\"]:\n",
    "        # Remove digits and special characters, keep only letters and spaces\n",
    "        cleaned_sentences.append(''.join(char for char in sentence if char.isalpha() or char.isspace()))\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d1e459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>! 00:01 Nestrapnuj sa krcmovymi frajerinami a ...</td>\n",
       "      <td>Nestrapnuj sa krcmovymi frajerinami a impote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007iwb, platený diskuter, ktorý v každej debat...</td>\n",
       "      <td>iwb platený diskuter ktorý v každej debate o K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01:15 - tvoj mozog plný fekálií je schopný iba...</td>\n",
       "      <td>tvoj mozog plný fekálií je schopný iba na pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01. apríl 2014, 07:18 Počet obetí zosuvu pôdy ...</td>\n",
       "      <td>apríl   Počet obetí zosuvu pôdy v štáte Washi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01. apríl 2014, 14:33 Nora Mojsejová na slobode?</td>\n",
       "      <td>apríl   Nora Mojsejová na slobode</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  ! 00:01 Nestrapnuj sa krcmovymi frajerinami a ...   \n",
       "1  007iwb, platený diskuter, ktorý v každej debat...   \n",
       "2  01:15 - tvoj mozog plný fekálií je schopný iba...   \n",
       "3  01. apríl 2014, 07:18 Počet obetí zosuvu pôdy ...   \n",
       "4   01. apríl 2014, 14:33 Nora Mojsejová na slobode?   \n",
       "\n",
       "                                             cleaned  \n",
       "0    Nestrapnuj sa krcmovymi frajerinami a impote...  \n",
       "1  iwb platený diskuter ktorý v každej debate o K...  \n",
       "2    tvoj mozog plný fekálií je schopný iba na pr...  \n",
       "3   apríl   Počet obetí zosuvu pôdy v štáte Washi...  \n",
       "4                  apríl   Nora Mojsejová na slobode  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean the sentences and add as a new column\n",
    "slk_df[\"cleaned\"] = produce_clean_sentence_list(slk_df)\n",
    "display(slk_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = slk_df[\"cleaned\"].astype(str).tolist()\n",
    "\n",
    "# separate into training and testing sets\n",
    "random.seed(42)\n",
    "random.shuffle(sentences)\n",
    "split_index = int(0.9 * len(sentences))\n",
    "train_sentences = sentences[:split_index]\n",
    "test_sentences = sentences[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tokenizer\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\", byte_fallback=True))\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(),\n",
    "    normalizers.Lowercase()\n",
    "])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=50_000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=special_tokens,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5ee650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenizer training\n",
    "tokenizer.train_from_iterator(train_sentences, trainer=trainer)\n",
    "\n",
    "# runtime options\n",
    "tokenizer.enable_padding(\n",
    "    pad_id=tokenizer.token_to_id(\"[PAD]\"),\n",
    "    pad_token=\"[PAD]\"\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b422ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_per_word_from_encoding(enc):\n",
    "    word_ids = None\n",
    "    if hasattr(enc, \"words\") and enc.words is not None:\n",
    "        word_ids = enc.words\n",
    "    elif hasattr(enc, \"word_ids\"):\n",
    "        try:\n",
    "            word_ids = enc.word_ids  # property\n",
    "        except TypeError:\n",
    "            word_ids = enc.word_ids()  # method\n",
    "\n",
    "    if word_ids:\n",
    "        # Filter out specials (None / -1 depending on impl)\n",
    "        valid_ids = [w for w in word_ids if w is not None and w != -1]\n",
    "        if not valid_ids:\n",
    "            return 0.0\n",
    "        # Count tokens per word index\n",
    "        per_word_counts = Counter(valid_ids)\n",
    "        return sum(per_word_counts.values()) / len(per_word_counts)\n",
    "\n",
    "    # approximate words via whitespace\n",
    "    text = enc.normalized_str if hasattr(enc, \"normalized_str\") else None\n",
    "    if text is None and hasattr(enc, \"tokens\"):\n",
    "        text = \" \".join(enc.tokens)\n",
    "    if text is None:\n",
    "        return 0.0\n",
    "    approx_words = [w for w in text.strip().split() if w]\n",
    "    return (len(enc.ids) / len(approx_words)) if approx_words else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0ae10f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token length in test set: 31.49\n",
      "Top 10 most frequent tokens (ID: Frequency):\n",
      "261: 464160, (Ìģ)\n",
      "262: 327280, (ÌĮ)\n",
      "282: 45914, (Ġna)\n",
      "265: 39286, (Ġv)\n",
      "269: 35731, (Ġa)\n",
      "225: 33373, (Ġ)\n",
      "303: 32287, (Ġsa)\n",
      "268: 29916, (Ġz)\n",
      "263: 28257, (Ġs)\n",
      "277: 24837, (ch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_137295/2298898216.py:8: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
      "  if hasattr(enc, \"words\") and enc.words is not None:\n",
      "/tmp/ipykernel_137295/2298898216.py:9: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
      "  word_ids = enc.words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per word in test set: 1.05\n"
     ]
    }
   ],
   "source": [
    "encoded_tests = [tokenizer.encode(sentence) for sentence in test_sentences]\n",
    "total_tokens = sum(len(enc.ids) for enc in encoded_tests)\n",
    "average_token_length = total_tokens / len(encoded_tests)\n",
    "print(f\"Average token length in test set: {average_token_length:.2f}\")\n",
    "\n",
    "# Token frequency distribution\n",
    "token_freq = {}\n",
    "for enc in encoded_tests:\n",
    "    for token_id in enc.ids:\n",
    "        token_freq[token_id] = token_freq.get(token_id, 0) + 1\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_token_freq = sorted(token_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 most frequent tokens (ID: Frequency):\")\n",
    "for token_id, freq in sorted_token_freq[:10]:\n",
    "    print(f\"{token_id}: {freq}, ({tokenizer.id_to_token(token_id)})\")\n",
    "\n",
    "# Number of tokens per word\n",
    "tokens_per_word_list = [tokens_per_word_from_encoding(enc) for enc in encoded_tests]\n",
    "average_tokens_per_word = sum(tokens_per_word_list) / len(tokens_per_word_list)\n",
    "print(f\"Average number of tokens per word in test set: {average_tokens_per_word:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
