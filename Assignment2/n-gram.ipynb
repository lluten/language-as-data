{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95dc840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import spacy\n",
    "\n",
    "from IPython.display import display\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c8ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from data/preprocessed/\n",
    "with open(\"../data/preprocessed/PROCESSED_slk_newscrawl_2016_1M-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    slk_df = pd.DataFrame(\n",
    "        [line.strip() for line in f.readlines()], columns=[\"sentence\"]\n",
    "    )\n",
    "with open(\"../data/preprocessed/PROCESSED_tur_news_2024_1M-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tur_df = pd.DataFrame(\n",
    "        [line.strip() for line in f.readlines()], columns=[\"sentence\"]\n",
    "    )\n",
    "\n",
    "# sanity check English wikipedia data\n",
    "with open(\"../data/preprocessed/PROCESSED_eng-simple_wikipedia_2021_300K-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eng_wiki_df = pd.DataFrame(\n",
    "        [line.strip() for line in f.readlines()], columns=[\"sentence\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0701accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toto byť testovací slovenský text .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joni/Documents/language-as-data/.310venv/lib/python3.10/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"sk_dep_web_md\")\n",
    "\n",
    "doc = nlp(\"Toto je testovací slovenský text.\")\n",
    "\n",
    "sentence = ' '.join([token.lemma_ for token in doc])\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ad3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_unk(corpus, min_freq=2):\n",
    "    \"\"\"\n",
    "    Replace hapax legomena with <unk> token.\n",
    "    Uses training corpus only for freq counting.\n",
    "    \"\"\"\n",
    "    # Count frequencies on the full training data\n",
    "    counts = Counter()\n",
    "    for sentence in corpus['sentence']:\n",
    "        counts.update(sentence.split())\n",
    "\n",
    "    # Build vocabulary\n",
    "    vocab = {w for w, c in counts.items() if c >= min_freq}\n",
    "    vocab.add(\"<unk>\")\n",
    "\n",
    "    # Replace rare words with <unk>\n",
    "    new_sentences = []\n",
    "    for sentence in corpus['sentence']:\n",
    "        tokens = [\n",
    "            w if w in vocab else \"<unk>\"\n",
    "            for w in sentence.split()\n",
    "        ]\n",
    "        new_sentences.append(\" \".join(tokens))\n",
    "\n",
    "    return {\"sentence\": new_sentences}, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e75c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "slk_train_df, slk_test_df = train_test_split(slk_df, test_size=0.1, random_state=42)\n",
    "tur_train_df, tur_test_df = train_test_split(tur_df, test_size=0.1, random_state=42)\n",
    "\n",
    "eng_train_df, eng_test_df = train_test_split(eng_wiki_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfcc8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trigram_model(corpus):\n",
    "    \"\"\"\n",
    "    Train a trigram model with counts for:\n",
    "      - trigram counts\n",
    "      - bigram counts (for conditioning)\n",
    "    \"\"\"\n",
    "    trigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "\n",
    "    for sentence in corpus['sentence']:\n",
    "        tokens = sentence.split()\n",
    "        for i in range(len(tokens) - 2):\n",
    "            w1, w2, w3 = tokens[i], tokens[i+1], tokens[i+2]\n",
    "            trigram_counts[(w1, w2, w3)] += 1\n",
    "            bigram_counts[(w1, w2)] += 1\n",
    "\n",
    "    return trigram_counts, bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f0a65f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(trigram_counts, bigram_counts, corpus, vocab):\n",
    "    \"\"\"\n",
    "    Trigram perplexity with add-one smoothing:\n",
    "    P(w3 | w1, w2) = (count(w1,w2,w3) + 1) / (count(w1,w2) + V)\n",
    "    \"\"\"\n",
    "    V = len(vocab)\n",
    "\n",
    "    log_prob_sum = 0.0\n",
    "    N = 0\n",
    "\n",
    "    for sentence in corpus['sentence']:\n",
    "        tokens = sentence.split()\n",
    "        for i in range(len(tokens) - 2):\n",
    "            w1, w2, w3 = tokens[i], tokens[i+1], tokens[i+2]\n",
    "\n",
    "            count_tri = trigram_counts.get((w1, w2, w3), 0)\n",
    "            count_bi = bigram_counts.get((w1, w2), 0)\n",
    "\n",
    "            prob = (count_tri + 1) / (count_bi + V)\n",
    "\n",
    "            log_prob_sum += math.log(prob)\n",
    "            N += 1\n",
    "\n",
    "    return math.exp(-log_prob_sum / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4283a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess train with <unk>\n",
    "slk_trained_processed, slk_vocab = preprocess_with_unk(slk_train_df, min_freq=2)\n",
    "tur_trained_processed, tur_vocab = preprocess_with_unk(tur_train_df, min_freq=2)\n",
    "\n",
    "eng_trained_processed, eng_vocab = preprocess_with_unk(eng_train_df, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03837d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess test set to replace OOV with <unk>\n",
    "def preprocess_test_set(test_df, vocab):\n",
    "    test_df_processed, _ = preprocess_with_unk(test_df)\n",
    "    test_df_processed[\"sentence\"] = [\n",
    "        \" \".join([(w if w in vocab else \"<unk>\")\n",
    "                for w in sent.split()])\n",
    "        for sent in test_df_processed[\"sentence\"]\n",
    "    ]\n",
    "    return test_df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ae774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slovak Trigram Model Perplexity: 117241.81\n",
      "Turkish Trigram Model Perplexity: 119828.92\n",
      "English Trigram Model Perplexity: 20566.69\n"
     ]
    }
   ],
   "source": [
    "# Train, evaluate, and print perplexities\n",
    "for lang, train_processed, test_processed, vocab in [\n",
    "    (\"Slovak\", slk_trained_processed, slk_test_df, slk_vocab),\n",
    "    (\"Turkish\", tur_trained_processed, tur_test_df, tur_vocab),\n",
    "    (\"English\", eng_trained_processed, eng_test_df, eng_vocab),\n",
    "]:\n",
    "    trigram_counts, bigram_counts = train_trigram_model(train_processed)\n",
    "    test_processed = preprocess_test_set(test_processed, vocab)\n",
    "    perplexity = calculate_perplexity(trigram_counts, bigram_counts, test_processed, vocab)\n",
    "    print(f\"{lang} Trigram Model Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d21729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slovak Most Common Trigrams:\n",
      "  prečítajte si exkluzívne: 12673\n",
      "  si exkluzívne prognózy: 12673\n",
      "  exkluzívne prognózy trendu: 12673\n",
      "  do diskusie sa: 9027\n",
      "  príspevku do diskusie: 9025\n",
      "Turkish Most Common Trigrams:\n",
      "  sosyal medya hesabından: 2498\n",
      "  bir kez daha: 2315\n",
      "  Cumhurbaşkanı Recep Tayyip: 2229\n",
      "  Recep Tayyip Erdoğan: 2157\n",
      "  Büyükşehir Belediye Başkanı: 1594\n",
      "English Most Common Trigrams:\n",
      "  one of the: 2787\n",
      "  part of the: 1655\n",
      "  the united states: 1555\n",
      "  it is the: 1167\n",
      "  <unk> <unk> <unk>: 1153\n"
     ]
    }
   ],
   "source": [
    "# print common trigrams\n",
    "for lang, train_processed in [\n",
    "    (\"Slovak\", slk_trained_processed),\n",
    "    (\"Turkish\", tur_trained_processed),\n",
    "    (\"English\", eng_trained_processed),\n",
    "]:\n",
    "    trigram_counts, _ = train_trigram_model(train_processed)\n",
    "    common_trigrams = trigram_counts.most_common(5)\n",
    "    print(f\"{lang} Most Common Trigrams:\")\n",
    "    for trigram, count in common_trigrams:\n",
    "        print(f\"  {' '.join(trigram)}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b431cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".310venv (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
