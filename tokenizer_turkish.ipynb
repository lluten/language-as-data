{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a409818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers import models, trainers, normalizers, pre_tokenizers, processors, decoders\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f65db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tur_news_2024_1M/tur_news_2024_1M-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tur_df = pd.DataFrame(\n",
    "            [line.strip().split(\"\\t\")[1] for line in f.readlines()], columns=[\"sentence\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea9bf1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function applied to dataset cleans sentences from digits and special characters\n",
    "def produce_clean_sentence_list(df):\n",
    "    cleaned_sentences = []\n",
    "    for sentence in df[\"sentence\"]:\n",
    "        # Remove digits and special characters, keep only letters and spaces\n",
    "        cleaned_sentences.append(''.join(char for char in sentence if char.isalpha() or char.isspace()))\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c22ba4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>%0.1 çok düşük çok düşük bir büyüme.</td>\n",
       "      <td>çok düşük çok düşük bir büyüme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"01 Kasım 2024 tarihinden itibaren geçerli olm...</td>\n",
       "      <td>Kasım  tarihinden itibaren geçerli olmak üzer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"02.04.2024 günü saat 12.47’de Beşiktaş ilçesi...</td>\n",
       "      <td>günü saat de Beşiktaş ilçesi Gayrettepe Mahal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"02 Adana Yasin-Poyraz\" firmasının et dönerind...</td>\n",
       "      <td>Adana YasinPoyraz firmasının et dönerinde der...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‘ 0-4 yaş arası çocuklu annelere, ‘Anne Kart’ ...</td>\n",
       "      <td>yaş arası çocuklu annelere Anne Kart uygulam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0               %0.1 çok düşük çok düşük bir büyüme.   \n",
       "1  \"01 Kasım 2024 tarihinden itibaren geçerli olm...   \n",
       "2  \"02.04.2024 günü saat 12.47’de Beşiktaş ilçesi...   \n",
       "3  \"02 Adana Yasin-Poyraz\" firmasının et dönerind...   \n",
       "4  ‘ 0-4 yaş arası çocuklu annelere, ‘Anne Kart’ ...   \n",
       "\n",
       "                                             cleaned  \n",
       "0                     çok düşük çok düşük bir büyüme  \n",
       "1   Kasım  tarihinden itibaren geçerli olmak üzer...  \n",
       "2   günü saat de Beşiktaş ilçesi Gayrettepe Mahal...  \n",
       "3   Adana YasinPoyraz firmasının et dönerinde der...  \n",
       "4    yaş arası çocuklu annelere Anne Kart uygulam...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean the sentences and add as a new column\n",
    "tur_df[\"cleaned\"] = produce_clean_sentence_list(tur_df)\n",
    "display(tur_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0da67fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tur_df[\"cleaned\"].astype(str).tolist()\n",
    "\n",
    "# separate into training and testing sets\n",
    "random.seed(42)\n",
    "random.shuffle(sentences)\n",
    "split_index = int(0.9 * len(sentences))\n",
    "train_sentences = sentences[:split_index]\n",
    "test_sentences = sentences[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04d4852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tokenizer\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\", byte_fallback=True))\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(),\n",
    "    normalizers.Lowercase()\n",
    "])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=50_000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=special_tokens,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9b968ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenizer training\n",
    "tokenizer.train_from_iterator(train_sentences, trainer=trainer)\n",
    "\n",
    "# runtime options\n",
    "tokenizer.enable_padding(\n",
    "    pad_id=tokenizer.token_to_id(\"[PAD]\"),\n",
    "    pad_token=\"[PAD]\"\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "507bfbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_per_word_from_encoding(enc):\n",
    "    word_ids = None\n",
    "    #if hasattr(enc, \"words\") and enc.words is not None:\n",
    "        #word_ids = enc.words\n",
    "    if hasattr(enc, \"ids\"):\n",
    "        try:\n",
    "            word_ids = enc.ids  # property\n",
    "        except TypeError:\n",
    "            word_ids = enc.ids()  # method\n",
    "\n",
    "    if word_ids:\n",
    "        # Filter out specials (None / -1 depending on impl)\n",
    "        valid_ids = [w for w in word_ids if w is not None and w != -1]\n",
    "        if not valid_ids:\n",
    "            return 0.0\n",
    "        # Count tokens per word index\n",
    "        per_word_counts = Counter(valid_ids)\n",
    "        return sum(per_word_counts.values()) / len(per_word_counts)\n",
    "\n",
    "    # approximate words via whitespace\n",
    "    text = enc.normalized_str if hasattr(enc, \"normalized_str\") else None\n",
    "    if text is None and hasattr(enc, \"tokens\"):\n",
    "        text = \" \".join(enc.tokens)\n",
    "    if text is None:\n",
    "        return 0.0\n",
    "    approx_words = [w for w in text.strip().split() if w]\n",
    "    return (len(enc.ids) / len(approx_words)) if approx_words else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cb4ec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token length in test set: 25.89\n",
      "Top 10 most frequent tokens (ID: Frequency):\n",
      "262: 232111, (Ì§)\n",
      "263: 226239, (ÌĪ)\n",
      "273: 93975, (ÌĨ)\n",
      "305: 33962, (Ġve)\n",
      "225: 33079, (Ġ)\n",
      "296: 31612, (Ġc)\n",
      "279: 25564, (Ġo)\n",
      "311: 23210, (Ġbir)\n",
      "307: 21840, (Ġbu)\n",
      "343: 17823, (Ìĩ)\n",
      "Average number of tokens per word in test set: 1.17\n"
     ]
    }
   ],
   "source": [
    "encoded_tests = [tokenizer.encode(sentence) for sentence in test_sentences]\n",
    "total_tokens = sum(len(enc.ids) for enc in encoded_tests)\n",
    "average_token_length = total_tokens / len(encoded_tests)\n",
    "print(f\"Average token length in test set: {average_token_length:.2f}\")\n",
    "\n",
    "# Token frequency distribution\n",
    "token_freq = {}\n",
    "for enc in encoded_tests:\n",
    "    for token_id in enc.ids:\n",
    "        token_freq[token_id] = token_freq.get(token_id, 0) + 1\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_token_freq = sorted(token_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 most frequent tokens (ID: Frequency):\")\n",
    "for token_id, freq in sorted_token_freq[:10]:\n",
    "    print(f\"{token_id}: {freq}, ({tokenizer.id_to_token(token_id)})\")\n",
    "\n",
    "# Number of tokens per word\n",
    "tokens_per_word_list = [tokens_per_word_from_encoding(enc) for enc in encoded_tests]\n",
    "average_tokens_per_word = sum(tokens_per_word_list) / len(tokens_per_word_list)\n",
    "print(f\"Average number of tokens per word in test set: {average_tokens_per_word:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23965f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
